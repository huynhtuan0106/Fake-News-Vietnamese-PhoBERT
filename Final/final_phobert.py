# -*- coding: utf-8 -*-
"""Final-PhoBERT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RXr4V96F3NFAQ6gjmhHCwfYz3G0CroEa
"""

import re
import numpy as np
from nltk.tokenize import RegexpTokenizer
from imblearn.over_sampling import RandomOverSampler
from google.colab import files
from transformers import AutoTokenizer, AutoModel
import torch
from sklearn.model_selection import train_test_split
from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments

# Xem các file đã tải lên
import pandas as pd
import io

# Đọc các file CSV vào DataFrame
real = pd.read_csv('new_real.csv')
fake = pd.read_csv('new_fake.csv')

real.shape

fake.shape

df = pd.concat([real, fake], ignore_index=True)
df.shape

df = df.dropna(subset=['label'])

df['label'] = df['label'].astype(int)

df.columns

df2 = df.drop(['link', 'comment_list', 'date', 'author_id'], axis=1)

df2.head()

df2['label'].value_counts().plot.bar()

def clean_text(text):
    # Chuyển đổi văn bản thành chữ thường
    text = text.lower()
    # Xóa các URL
    text = re.sub(r"http\S+", "", text)
    # Xóa các ký tự không phải là chữ cái, số hoặc dấu thanh cơ bản
    text = re.sub(r"[^a-zA-Z0-9\sáàảãạăằắẳẵặâầấẩẫậèẹẻẽẽêềếểễệìíỉĩịòóỏõọôồốổỗộơờởỡợớùúủũụưừửữựứỳýỷỹỵđ]", "", text)
    # Xóa các khoảng trắng thừa
    text = re.sub(r"\s+", " ", text).strip()

    return text

# Clean content
df2['content'] = df2['content'].apply(clean_text)

df2.head()

# Tạo bước tăng cường dữ liệu
ros = RandomOverSampler()
df2['content'] = df2['content'].astype(str)  # Đảm bảo cột nội dung là kiểu chuỗi
x_over, y_over = ros.fit_resample(df2[['content']], df2['label'])
df2 = pd.DataFrame(data=x_over, columns=['content'])
df2['label'] = y_over

# Kiểm tra phân phối nhãn
df2['label'].value_counts().plot.bar()

df2.shape

tokenizer = AutoTokenizer.from_pretrained("vinai/phobert-base")
model = AutoModel.from_pretrained("vinai/phobert-base")

# Chia dữ liệu thành tập huấn luyện và kiểm tra
train_texts, val_texts, train_labels, val_labels = train_test_split(df2['content'], df2['label'], test_size=0.2)

# Tokenize dữ liệu với padding
def tokenize_and_pad(texts, tokenizer, max_length=512):
    encodings = tokenizer(
        texts,
        truncation=True,
        padding='max_length',  # Thêm padding để tất cả các văn bản có cùng chiều dài
        max_length=max_length
    )
    return encodings

train_encodings = tokenize_and_pad(train_texts.tolist(), tokenizer, max_length=512)
val_encodings = tokenize_and_pad(val_texts.tolist(), tokenizer, max_length=512)

# Tokenize dữ liệu với cắt ngắn văn bản
train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True, max_length=256, truncation_strategy='longest_first')
val_encodings = tokenizer(val_texts.tolist(), truncation=True, padding=True, max_length=256, truncation_strategy='longest_first')

# Kiểm tra dữ liệu sau khi token hóa
print(train_encodings.keys())
print(val_encodings.keys())
print(train_encodings['input_ids'][:2])  # Hiển thị một vài mẫu tokenized
print(val_encodings['input_ids'][:2])

train_labels = np.array(train_labels).astype(int)
val_labels = np.array(val_labels).astype(int)

# Kiểm tra nhãn
print(set(train_labels))
print(set(val_labels))

import torch
from torch.utils.data import Dataset, DataLoader

class FakeNewsDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx]).squeeze()  # Đảm bảo nhãn có kích thước đúng
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = FakeNewsDataset(train_encodings, train_labels.tolist())
val_dataset = FakeNewsDataset(val_encodings, val_labels.tolist())

train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)

# Tải mô hình phân loại
model = AutoModelForSequenceClassification.from_pretrained("vinai/phobert-base", num_labels=2)

# Thiết lập các tham số huấn luyện
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
)

# Khởi tạo Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset
)

# Huấn luyện mô hình
trainer.train()

results = trainer.evaluate()
print(results)

predictions = trainer.predict(val_dataset)
print(predictions.metrics)

model.save_pretrained('./saved_model')
tokenizer.save_pretrained('./saved_model')

from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained('./saved_model')
tokenizer = AutoTokenizer.from_pretrained('./saved_model')

inputs = tokenizer("Sập hầm thang Quảng Ninh khiến con số thiệt mạng khổng lồ", return_tensors="pt")
# Thực hiện dự đoán
with torch.no_grad():
    outputs = model(**inputs)

# Lấy logits từ kết quả dự đoán
logits = outputs.logits
print(logits)

from torch.nn.functional import softmax

probs = softmax(logits, dim=1)
print(probs)

from torch.nn.functional import softmax

# Chuyển đổi logits thành xác suất
probs = softmax(logits, dim=1)
print(probs)

# Lấy lớp dự đoán (lớp có xác suất cao nhất)
predicted_class = torch.argmax(probs, dim=1)
print(predicted_class)